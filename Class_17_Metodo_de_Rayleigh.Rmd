---
title: "An√°lisis Num√©rico I, Primavera 2025"
subtitle: "An√°lisis Num√©rico, R. L. Burden y J. D. Faires, ed. 10."
author: "Instructor: Imelda Trejo"
date: "Last update: 2025-March-31"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide

---

# 5 Valores y Vectores propios

Este material no viene en el libro de clase,

https://heavyphysicsblog.wordpress.com/wp-content/uploads/2019/06/analisis-numerico-burden-faires-10ed.pdf

ver referencias al final del texto.


## 5.3 M√©todo de Rayleigh

Continuamos con m√©todos iterativos para estimar valores propios.

### 5.3.1 Teorema de Rayleigh

Sea $A$ una matriz hermitiana $n\times n$ con valores propios 

  \[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n\]
      
y sus correspondientes vectores propios ortogonales $v_1,..,v_n$ entre si.
Entonces 

$$\lambda_j=\max_{x\in X_j, \  x\neq 0}\frac{\langle Ax, x\rangle}{\langle x, x\rangle}$$
donde el subespacio $X_1$, ...,$X_n$  se definen como $X_1=\mathbf{C}^n$ y 
$$X_j=\{x\in  \mathbf{C}^n : \langle x, v_k\rangle=0, k=1,...,j-1\}$$
con $j=2,...,n$.


Demostraci√≥n (ejercicio).

Sea $x\in X_j$ con $x\neq 0$. Entonces


$$x=\sum^{n}_{k=j}\langle x, v_k\rangle v_k, \quad \langle x, x\rangle= \sum^{n}_{k=j}|\langle x, v_k\rangle|^2.$$
As√≠,
$$Ax=\sum^n_{k=j}\lambda_k\langle x,v_k \rangle v_k$$
$$\langle Ax,x \rangle=\sum^n_{k=j}\lambda_k|\langle x,v_k \rangle|^2\leq \lambda_j\sum^n_{k=j}|\langle x,v_k \rangle|^2=\lambda_j\langle x,x \rangle.$$
Se sigue que,

$$\frac{\langle Ax,x \rangle}{\langle x,x \rangle}\leq \lambda_j, \ \forall x\in X_j$$
As√≠, 
$$\sup_{x\in X_j, \  x\neq 0}\frac{\langle Ax, x\rangle}{\langle x, x\rangle} \leq \lambda_j.$$
Luego, este supremos se alcanza, porque $$\langle Av_j,v_j \rangle=\lambda_j \quad \text{con}\quad v_j\in X_j.$$

---

### 5.3.2 Introducci√≥n del M√©todo de Rayleigh

El **m√©todo de Rayleigh** permite estimar valores propios de una matriz cuadrada \(A\) de $n \times n$, especialmente si \(A\) es **simetrica**. La t√©cnica utiliza el **cociente de Rayleigh**, que para un vector \(x \neq 0\) se define como:

\[
\lambda_k = R(x_k) = \frac{x_k^T A x_k}{x_k^T x_k}
\]

Donde:

- \(x_k\) es un vector aproximado del autovector.
- \(A\) es una matriz sim√©trica.
- \(\lambda_k\) es la estimaci√≥n del valor propio asociado.

---

### 5.3.3 Procedimiento del M√©todo

###  **Paso 1: Inicializaci√≥n**

- Elige un vector inicial \(x_0\), usualmente normalizado.
- Calcula el cociente de Rayleigh inicial:

\[
\lambda_0 = \frac{x_0^T A x_0}{x_0^T x_0}
\]

### **Paso 2: Iteraci√≥n**

Para cada iteraci√≥n \(k\):

1. **Resolver el sistema lineal:**
\[
(A - \lambda_k I) y_k = x_k
\]
2. **Actualizar el vector:**
\[
x_{k+1} = \frac{y_k}{\|y_k\|}
\]
3. **Actualizar el valor propio:**
\[
\lambda_{k+1} = \frac{x_{k+1}^T A x_{k+1}}{x_{k+1}^T x_{k+1}}
\]

### **Paso 3: Criterio de Paro**
Detener la iteraci√≥n si:
\[
\|A x_k - \lambda_k x_k \| < \epsilon
\]
Donde \(\epsilon\) es la tolerancia deseada.

---

### 5.3.4 Ejemplo Resuelto Paso a Paso

Consideremos la matriz:

\[
A = \begin{pmatrix}
4 & 1 \\
1 & 3
\end{pmatrix}
\]

### **Paso 1: Vector inicial**

\[
x_0 = \begin{pmatrix}
1 \\
1
\end{pmatrix}, \quad \text{normalizado: } x_0 = \frac{1}{\sqrt{2}} \begin{pmatrix}
1 \\
1
\end{pmatrix}
\]

### **Paso 2: C√°lculo inicial del cociente de Rayleigh**

\[
\lambda_0 = \frac{x_0^T A x_0}{x_0^T x_0} = \frac{\frac{1}{2}(4 + 2 + 3)}{1} = 4.5
\]

### **Paso 3: Resoluci√≥n del sistema**

\[
(A - \lambda_0 I) y_0 = x_0 \implies \begin{pmatrix}
4 - 4.5 & 1 \\
1 & 3 - 4.5
\end{pmatrix} y_0 = x_0
\]

### **Paso 4: Actualizar vector y repetir iteraci√≥n**
Repetir hasta alcanzar la tolerancia deseada.



---

### 5.3.5 Implementaci√≥n en R

```{r}
# Definimos la matriz A
A <- matrix(c(6, 4, 4, 5), nrow = 2, byrow = TRUE)

# Vector inicial
x <- c(6, -7)
#x <- c(1, 1)

x <- x / sqrt(sum(x^2))  # Normalizamos

# Tolerancia y n√∫mero m√°ximo de iteraciones
tol <- 1e-2
max_iter <- 100
lambda_old <- 0

for (k in 1:max_iter) {
  lambda_new <- (t(x) %*% A %*% x / sum(x^2))[1,1]  # Cociente de Rayleigh

  
  # Check condition number
  kappa <- kappa(A -  lambda_new* diag(2))

  
    
  y <- solve(A - lambda_new * diag(2), x)    # Resolver sistema lineal
  x <- y / sqrt(sum(y^2))                    # Normalizar nuevo vector


  # Verificar convergencia

    if (abs(lambda_new - lambda_old) < tol || kappa>10^6 ) {
    cat("Convergencia alcanzada en iteracion:", k, "\n")
    cat("Valor propio estimado:", lambda_new, "\n")
    break
  }
  lambda_old <- lambda_new

}

#
print("Comprobado resultado usando eigen(A)")

eigen(A)
```

---

### 5.3.6. Observaciones Importantes

- Si la aproximaci√≥n inicial \(x_0\) est√° cerca de un autovector, el m√©todo converge muy r√°pidamente.
- Si \(A\) no es sim√©trica, el m√©todo puede no converger.
- El costo computacional es elevado debido a la necesidad de resolver sistemas lineales en cada iteraci√≥n.
- El sistema $(ùê¥-\lambda_k I) y_k=x_k$ puede volverse casi singular cuando el valor propio aproximado $\lambda_k$ en la iteraci√≥n 
$k$ se acerca demasiado a un valor propio de $A$.

---

### 5.3.7. Ejercicios para los Estudiantes

1. **Ejercicio 1:** Aplica el m√©todo de Rayleigh para la matriz:
\[
A = \begin{pmatrix}
6 & 2 \\
2 & 5
\end{pmatrix}
\]
Usa \(x_0 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\) como vector inicial.

2. **Ejercicio 2:** Implementa el m√©todo para una matriz de 3x3 y verifica la convergencia del m√©todo.

---

### 5.3.8. Recursos Adicionales

- Rainer Kress,  (1998), **Numerical Analysis**.
- Golub, G. H., & Van Loan, C. F. (2013). *Matrix Computations*.
- Trefethen, L. N., & Bau, D. (1997). *Numerical Linear Algebra*.




